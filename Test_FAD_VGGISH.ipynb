{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cpu \\\n",
        "  torch==2.9.1+cpu torchvision==0.24.1+cpu torchaudio==2.9.1+cpu resampy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLsRd0tSuizr",
        "outputId": "dc29f683-bbf2-4c66-cf73-75ee4aaa23f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch==2.9.1+cpu in /usr/local/lib/python3.12/dist-packages (2.9.1+cpu)\n",
            "Requirement already satisfied: torchvision==0.24.1+cpu in /usr/local/lib/python3.12/dist-packages (0.24.1+cpu)\n",
            "Requirement already satisfied: torchaudio==2.9.1+cpu in /usr/local/lib/python3.12/dist-packages (2.9.1+cpu)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.12/dist-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1+cpu) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1+cpu) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1+cpu) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1+cpu) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1+cpu) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1+cpu) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1+cpu) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.24.1+cpu) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.24.1+cpu) (11.3.0)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.12/dist-packages (from resampy) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.53->resampy) (0.43.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.1+cpu) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.1+cpu) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSKOkApdtmIo",
        "outputId": "21c06dbd-9d67-4664-b8f6-a8c9e7065bbf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "\n",
        "def add_noise(data, stddev):\n",
        "  \"\"\"Adds Gaussian noise to the samples.\n",
        "  Args:\n",
        "    data: 1d Numpy array containing floating point samples. Not necessarily\n",
        "      normalized.\n",
        "    stddev: The standard deviation of the added noise.\n",
        "  Returns:\n",
        "     1d Numpy array containing the provided floating point samples with added\n",
        "     Gaussian noise.\n",
        "  Raises:\n",
        "    ValueError: When data is not a 1d numpy array.\n",
        "  \"\"\"\n",
        "  if len(data.shape) != 1:\n",
        "    raise ValueError(\"expected 1d numpy array.\")\n",
        "  max_value = np.amax(np.abs(data))\n",
        "  num_samples = data.shape[0]\n",
        "  gauss = np.random.normal(0, stddev, (num_samples)) * max_value\n",
        "  return data + gauss\n",
        "\n",
        "\n",
        "def gen_sine_wave(freq=600,\n",
        "                  length_seconds=6,\n",
        "                  sample_rate=SAMPLE_RATE,\n",
        "                  param=None):\n",
        "  \"\"\"Creates sine wave of the specified frequency, sample_rate and length.\"\"\"\n",
        "  t = np.linspace(0, length_seconds, int(length_seconds * sample_rate))\n",
        "  samples = np.sin(2 * np.pi * t * freq)\n",
        "  if param:\n",
        "    samples = add_noise(samples, param)\n",
        "  return np.asarray(2**15 * samples, dtype=np.int16)"
      ],
      "metadata": {
        "id": "jt7schKFvspd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import resampy\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "\n",
        "def load_audio_task(fname, sample_rate, channels, dtype=\"float32\"):\n",
        "    if dtype not in ['float64', 'float32', 'int32', 'int16']:\n",
        "        raise ValueError(f\"dtype not supported: {dtype}\")\n",
        "\n",
        "    wav_data, sr = sf.read(fname, dtype=dtype)\n",
        "    # For integer type PCM input, convert to [-1.0, +1.0]\n",
        "    if dtype == 'int16':\n",
        "        wav_data = wav_data / 32768.0\n",
        "    elif dtype == 'int32':\n",
        "        wav_data = wav_data / float(2**31)\n",
        "\n",
        "    # Convert to mono\n",
        "    assert channels in [1, 2], \"channels must be 1 or 2\"\n",
        "    if len(wav_data.shape) > channels:\n",
        "        wav_data = np.mean(wav_data, axis=1)\n",
        "\n",
        "    if sr != sample_rate:\n",
        "        wav_data = resampy.resample(wav_data, sr, sample_rate)\n",
        "\n",
        "    return wav_data\n"
      ],
      "metadata": {
        "id": "NGBV6PD_v4dV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate Frechet Audio Distance betweeen two audio directories.\n",
        "\n",
        "Frechet distance implementation adapted from: https://github.com/mseitzer/pytorch-fid\n",
        "\n",
        "VGGish adapted from: https://github.com/harritaylor/torchvggish\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import resampy\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import laion_clap\n",
        "\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "from scipy import linalg\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class FrechetAudioDistance:\n",
        "    def __init__(\n",
        "        self,\n",
        "        ckpt_dir=None,\n",
        "        model_name=\"vggish\",\n",
        "        submodel_name=\"630k-audioset\",  # only for CLAP\n",
        "        sample_rate=16000,\n",
        "        channels=1,\n",
        "        use_pca=False,  # only for VGGish\n",
        "        use_activation=False,  # only for VGGish\n",
        "        verbose=False,\n",
        "        audio_load_worker=8,\n",
        "        enable_fusion=False,  # only for CLAP\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize FAD\n",
        "\n",
        "        -- ckpt_dir: folder where the downloaded checkpoints are stored\n",
        "        -- model_name: one between vggish, pann, clap or encodec\n",
        "        -- submodel_name: only for clap models - determines which checkpoint to use.\n",
        "                          options: [\"630k-audioset\", \"630k\", \"music_audioset\", \"music_speech\", \"music_speech_audioset\"]\n",
        "        -- sample_rate: one between [8000, 16000, 32000, 48000]. depending on the model set the sample rate to use\n",
        "        -- channels: number of channels in an audio track\n",
        "        -- use_pca: whether to apply PCA to the vggish embeddings\n",
        "        -- use_activation: whether to use the output activation in vggish\n",
        "        -- enable_fusion: whether to use fusion for clap models (valid depending on the specific submodel used)\n",
        "        \"\"\"\n",
        "        assert model_name in [\"vggish\", \"pann\", \"clap\", \"encodec\"], \"model_name must be either 'vggish', 'pann', 'clap' or 'encodec'\"\n",
        "        if model_name == \"vggish\":\n",
        "            assert sample_rate == 16000, \"sample_rate must be 16000\"\n",
        "        elif model_name == \"pann\":\n",
        "            assert sample_rate in [8000, 16000, 32000], \"sample_rate must be 8000, 16000 or 32000\"\n",
        "        elif model_name == \"clap\":\n",
        "            assert sample_rate == 48000, \"sample_rate must be 48000\"\n",
        "            assert submodel_name in [\"630k-audioset\", \"630k\", \"music_audioset\", \"music_speech\", \"music_speech_audioset\"]\n",
        "        elif model_name == \"encodec\":\n",
        "            assert sample_rate in [24000, 48000], \"sample_rate must be 24000 or 48000\"\n",
        "            if sample_rate == 48000:\n",
        "                assert channels == 2, \"channels must be 2 for 48khz encodec model\"\n",
        "        self.model_name = model_name\n",
        "        self.submodel_name = submodel_name\n",
        "        self.sample_rate = sample_rate\n",
        "        self.channels = channels\n",
        "        self.verbose = verbose\n",
        "        self.device = torch.device(\n",
        "            'cuda') if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
        "        if self.device == torch.device('mps') and self.model_name == \"clap\":\n",
        "            if self.verbose:\n",
        "                print(\"[Frechet Audio Distance] CLAP does not support MPS device yet, because:\")\n",
        "                print(\"[Frechet Audio Distance] The operator 'aten::upsample_bicubic2d.out' is not currently implemented for the MPS device.\")\n",
        "                print(\"[Frechet Audio Distance] Using CPU device instead.\")\n",
        "            self.device = torch.device('cpu')\n",
        "        if self.verbose:\n",
        "            print(\"[Frechet Audio Distance] Using device: {}\".format(self.device))\n",
        "        self.audio_load_worker = audio_load_worker\n",
        "        self.enable_fusion = enable_fusion\n",
        "        if ckpt_dir is not None:\n",
        "            os.makedirs(ckpt_dir, exist_ok=True)\n",
        "            torch.hub.set_dir(ckpt_dir)\n",
        "            self.ckpt_dir = ckpt_dir\n",
        "        else:\n",
        "            # by default `ckpt_dir` is `torch.hub.get_dir()`\n",
        "            self.ckpt_dir = torch.hub.get_dir()\n",
        "        self.__get_model(model_name=model_name, use_pca=use_pca, use_activation=use_activation)\n",
        "\n",
        "    def __get_model(self, model_name=\"vggish\", use_pca=False, use_activation=False):\n",
        "        \"\"\"\n",
        "        Get ckpt and set model for the specified model_name\n",
        "\n",
        "        Params:\n",
        "        -- model_name: one between vggish, pann or clap\n",
        "        -- use_pca: whether to apply PCA to the vggish embeddings\n",
        "        -- use_activation: whether to use the output activation in vggish\n",
        "        \"\"\"\n",
        "        # vggish\n",
        "        if model_name == \"vggish\":\n",
        "            # S. Hershey et al., \"CNN Architectures for Large-Scale Audio Classification\", ICASSP 2017\n",
        "            self.model = torch.hub.load(repo_or_dir='harritaylor/torchvggish', model='vggish')\n",
        "            if not use_pca:\n",
        "                self.model.postprocess = False\n",
        "            if not use_activation:\n",
        "                self.model.embeddings = nn.Sequential(*list(self.model.embeddings.children())[:-1])\n",
        "            self.model.device = self.device\n",
        "\n",
        "        # pann\n",
        "        elif model_name == \"pann\":\n",
        "            # Kong et al., \"PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition\", IEEE/ACM Transactions on Audio, Speech, and Language Processing 28 (2020)\n",
        "\n",
        "            # choose the right checkpoint and model based on sample_rate\n",
        "            if self.sample_rate == 8000:\n",
        "                download_name = \"Cnn14_8k_mAP%3D0.416.pth\"\n",
        "                self.model = Cnn14_8k(\n",
        "                    sample_rate=8000,\n",
        "                    window_size=256,\n",
        "                    hop_size=80,\n",
        "                    mel_bins=64,\n",
        "                    fmin=50,\n",
        "                    fmax=4000,\n",
        "                    classes_num=527\n",
        "                )\n",
        "            elif self.sample_rate == 16000:\n",
        "                download_name = \"Cnn14_16k_mAP%3D0.438.pth\"\n",
        "                self.model = Cnn14_16k(\n",
        "                    sample_rate=16000,\n",
        "                    window_size=512,\n",
        "                    hop_size=160,\n",
        "                    mel_bins=64,\n",
        "                    fmin=50,\n",
        "                    fmax=8000,\n",
        "                    classes_num=527\n",
        "                )\n",
        "            elif self.sample_rate == 32000:\n",
        "                download_name = \"Cnn14_mAP%3D0.431.pth\"\n",
        "                self.model = Cnn14(\n",
        "                    sample_rate=32000,\n",
        "                    window_size=1024,\n",
        "                    hop_size=320,\n",
        "                    mel_bins=64,\n",
        "                    fmin=50,\n",
        "                    fmax=16000,\n",
        "                    classes_num=527\n",
        "                )\n",
        "\n",
        "            model_path = os.path.join(self.ckpt_dir, download_name)\n",
        "\n",
        "            # download checkpoint\n",
        "            if not (os.path.exists(model_path)):\n",
        "                if self.verbose:\n",
        "                    print(\"[Frechet Audio Distance] Downloading {}...\".format(model_path))\n",
        "                torch.hub.download_url_to_file(\n",
        "                    url=f\"https://zenodo.org/record/3987831/files/{download_name}\",\n",
        "                    dst=model_path\n",
        "                )\n",
        "\n",
        "            # load checkpoint\n",
        "            checkpoint = torch.load(model_path, map_location=self.device)\n",
        "            self.model.load_state_dict(checkpoint['model'])\n",
        "        # clap\n",
        "        elif model_name == \"clap\":\n",
        "            # choose the right checkpoint and model\n",
        "            if self.submodel_name == \"630k-audioset\":\n",
        "                if self.enable_fusion:\n",
        "                    download_name = \"630k-audioset-fusion-best.pt\"\n",
        "                else:\n",
        "                    download_name = \"630k-audioset-best.pt\"\n",
        "            elif self.submodel_name == \"630k\":\n",
        "                if self.enable_fusion:\n",
        "                    download_name = \"630k-fusion-best.pt\"\n",
        "                else:\n",
        "                    download_name = \"630k-best.pt\"\n",
        "            elif self.submodel_name == \"music_audioset\":\n",
        "                download_name = \"music_audioset_epoch_15_esc_90.14.pt\"\n",
        "            elif self.submodel_name == \"music_speech\":\n",
        "                download_name = \"music_speech_epoch_15_esc_89.25.pt\"\n",
        "            elif self.submodel_name == \"music_speech_audioset\":\n",
        "                download_name = \"music_speech_audioset_epoch_15_esc_89.98.pt\"\n",
        "\n",
        "            model_path = os.path.join(self.ckpt_dir, download_name)\n",
        "\n",
        "            # download checkpoint\n",
        "            if not (os.path.exists(model_path)):\n",
        "                if self.verbose:\n",
        "                    print(\"[Frechet Audio Distance] Downloading {}...\".format(model_path))\n",
        "                torch.hub.download_url_to_file(\n",
        "                    url=f\"https://huggingface.co/lukewys/laion_clap/resolve/main/{download_name}\",\n",
        "                    dst=model_path\n",
        "                )\n",
        "            # init model and load checkpoint\n",
        "            if self.submodel_name in [\"630k-audioset\", \"630k\"]:\n",
        "                self.model = laion_clap.CLAP_Module(enable_fusion=self.enable_fusion,\n",
        "                                                    device=self.device)\n",
        "            elif self.submodel_name in [\"music_audioset\", \"music_speech\", \"music_speech_audioset\"]:\n",
        "                self.model = laion_clap.CLAP_Module(enable_fusion=self.enable_fusion,\n",
        "                                                    amodel='HTSAT-base',\n",
        "                                                    device=self.device)\n",
        "            self.model.load_ckpt(model_path)\n",
        "\n",
        "            # init model and load checkpoint\n",
        "            if self.submodel_name in [\"630k-audioset\", \"630k\"]:\n",
        "                self.model = laion_clap.CLAP_Module(enable_fusion=self.enable_fusion,\n",
        "                                                    device=self.device)\n",
        "            elif self.submodel_name in [\"music_audioset\", \"music_speech\", \"music_speech_audioset\"]:\n",
        "                self.model = laion_clap.CLAP_Module(enable_fusion=self.enable_fusion,\n",
        "                                                    amodel='HTSAT-base',\n",
        "                                                    device=self.device)\n",
        "            self.model.load_ckpt(model_path)\n",
        "\n",
        "        # encodec\n",
        "        elif model_name == \"encodec\":\n",
        "            # choose the right model based on sample_rate\n",
        "            # weights are loaded from the encodec repo: https://github.com/facebookresearch/encodec/\n",
        "            if self.sample_rate == 24000:\n",
        "                self.model = EncodecModel.encodec_model_24khz()\n",
        "            elif self.sample_rate == 48000:\n",
        "                self.model = EncodecModel.encodec_model_48khz()\n",
        "            # 24kbps is the max bandwidth supported by both versions\n",
        "            # these models use 32 residual quantizers\n",
        "            self.model.set_target_bandwidth(24.0)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_embeddings(self, x, sr):\n",
        "        \"\"\"\n",
        "        Get embeddings using VGGish, PANN, CLAP or EnCodec models.\n",
        "        Params:\n",
        "        -- x    : a list of np.ndarray audio samples\n",
        "        -- sr   : sampling rate.\n",
        "        \"\"\"\n",
        "        embd_lst = []\n",
        "        try:\n",
        "            for audio in tqdm(x, disable=(not self.verbose)):\n",
        "                if self.model_name == \"vggish\":\n",
        "                    embd = self.model.forward(audio, sr)\n",
        "\n",
        "                    # if SAMPLE_RATE is 48000, we need to make audio stereo\n",
        "                    if self.model.sample_rate == 48000:\n",
        "                        if audio.shape[-1] != 2:\n",
        "                            if self.verbose:\n",
        "                                print(\n",
        "                                    \"[Frechet Audio Distance] Audio is mono, converting to stereo for 48khz model...\"\n",
        "                                )\n",
        "                            audio = torch.cat((audio, audio), dim=1)\n",
        "                        else:\n",
        "                            # transpose to (batch, channels, samples)\n",
        "                            audio = audio[:, 0].transpose(1, 2)\n",
        "\n",
        "                    if self.verbose:\n",
        "                        print(\n",
        "                            \"[Frechet Audio Distance] Audio shape: {}\".format(\n",
        "                                audio.shape\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        # encodec embedding (before quantization)\n",
        "                        embd = self.model.encoder(audio)\n",
        "                        embd = embd.squeeze(0)\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(\n",
        "                        \"[Frechet Audio Distance] Embedding shape: {}\".format(\n",
        "                            embd.shape\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                if embd.device != torch.device(\"cpu\"):\n",
        "                    embd = embd.cpu()\n",
        "\n",
        "                if torch.is_tensor(embd):\n",
        "                    embd = embd.detach().numpy()\n",
        "\n",
        "                embd_lst.append(embd)\n",
        "        except Exception as e:\n",
        "            print(\"[Frechet Audio Distance] get_embeddings throw an exception: {}\".format(str(e)))\n",
        "\n",
        "        return np.concatenate(embd_lst, axis=0)\n",
        "\n",
        "    def calculate_embd_statistics(self, embd_lst):\n",
        "        if isinstance(embd_lst, list):\n",
        "            embd_lst = np.array(embd_lst)\n",
        "        mu = np.mean(embd_lst, axis=0)\n",
        "        sigma = np.cov(embd_lst, rowvar=False)\n",
        "        return mu, sigma\n",
        "\n",
        "    def calculate_frechet_distance(self, mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "        \"\"\"\n",
        "        Adapted from: https://github.com/mseitzer/pytorch-fid/blob/master/src/pytorch_fid/fid_score.py\n",
        "\n",
        "        Numpy implementation of the Frechet Distance.\n",
        "        The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "        and X_2 ~ N(mu_2, C_2) is\n",
        "                d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "        Stable version by Dougal J. Sutherland.\n",
        "        Params:\n",
        "        -- mu1   : Numpy array containing the activations of a layer of the\n",
        "                inception net (like returned by the function 'get_predictions')\n",
        "                for generated samples.\n",
        "        -- mu2   : The sample mean over activations, precalculated on an\n",
        "                representative data set.\n",
        "        -- sigma1: The covariance matrix over activations for generated samples.\n",
        "        -- sigma2: The covariance matrix over activations, precalculated on an\n",
        "                representative data set.\n",
        "        Returns:\n",
        "        --   : The Frechet Distance.\n",
        "        \"\"\"\n",
        "\n",
        "        mu1 = np.atleast_1d(mu1)\n",
        "        mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "        sigma1 = np.atleast_2d(sigma1)\n",
        "        sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "        assert mu1.shape == mu2.shape, \\\n",
        "            'Training and test mean vectors have different lengths'\n",
        "        assert sigma1.shape == sigma2.shape, \\\n",
        "            'Training and test covariances have different dimensions'\n",
        "\n",
        "        diff = mu1 - mu2\n",
        "\n",
        "        # Product might be almost singular\n",
        "        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2).astype(complex), disp=False)\n",
        "        if not np.isfinite(covmean).all():\n",
        "            msg = ('fid calculation produces singular product; '\n",
        "                   'adding %s to diagonal of cov estimates') % eps\n",
        "            print(msg)\n",
        "            offset = np.eye(sigma1.shape[0]) * eps\n",
        "            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset).astype(complex))\n",
        "\n",
        "        # Numerical error might give slight imaginary component\n",
        "        if np.iscomplexobj(covmean):\n",
        "            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "                m = np.max(np.abs(covmean.imag))\n",
        "                raise ValueError('Imaginary component {}'.format(m))\n",
        "            covmean = covmean.real\n",
        "\n",
        "        tr_covmean = np.trace(covmean)\n",
        "\n",
        "        return (diff.dot(diff) + np.trace(sigma1)\n",
        "                + np.trace(sigma2) - 2 * tr_covmean)\n",
        "\n",
        "    def __load_audio_files(self, dir, dtype=\"float32\"):\n",
        "        task_results = []\n",
        "\n",
        "        pool = ThreadPool(self.audio_load_worker)\n",
        "        pbar = tqdm(total=len(os.listdir(dir)), disable=(not self.verbose))\n",
        "\n",
        "        def update(*a):\n",
        "            pbar.update()\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"[Frechet Audio Distance] Loading audio from {}...\".format(dir))\n",
        "        for fname in os.listdir(dir):\n",
        "            res = pool.apply_async(\n",
        "                load_audio_task,\n",
        "                args=(os.path.join(dir, fname), self.sample_rate, self.channels, dtype),\n",
        "                callback=update,\n",
        "            )\n",
        "            task_results.append(res)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "        return [k.get() for k in task_results]\n",
        "\n",
        "    def score(self,\n",
        "              background_dir,\n",
        "              eval_dir,\n",
        "              background_embds_path=None,\n",
        "              eval_embds_path=None,\n",
        "              dtype=\"float32\"\n",
        "              ):\n",
        "        \"\"\"\n",
        "        Computes the Frechet Audio Distance (FAD) between two directories of audio files.\n",
        "\n",
        "        Parameters:\n",
        "        - background_dir (str): Path to the directory containing background audio files.\n",
        "        - eval_dir (str): Path to the directory containing evaluation audio files.\n",
        "        - background_embds_path (str, optional): Path to save/load background audio embeddings (e.g., /folder/bkg_embs.npy). If None, embeddings won't be saved.\n",
        "        - eval_embds_path (str, optional): Path to save/load evaluation audio embeddings (e.g., /folder/test_embs.npy). If None, embeddings won't be saved.\n",
        "        - dtype (str, optional): Data type for loading audio. Default is \"float32\".\n",
        "\n",
        "        Returns:\n",
        "        - float: The Frechet Audio Distance (FAD) score between the two directories of audio files.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load or compute background embeddings\n",
        "            if background_embds_path is not None and os.path.exists(background_embds_path):\n",
        "                if self.verbose:\n",
        "                    print(f\"[Frechet Audio Distance] Loading embeddings from {background_embds_path}...\")\n",
        "                embds_background = np.load(background_embds_path)\n",
        "            else:\n",
        "                audio_background = self.__load_audio_files(background_dir, dtype=dtype)\n",
        "                embds_background = self.get_embeddings(audio_background, sr=self.sample_rate)\n",
        "                if background_embds_path:\n",
        "                    os.makedirs(os.path.dirname(background_embds_path), exist_ok=True)\n",
        "                    np.save(background_embds_path, embds_background)\n",
        "\n",
        "            # Load or compute eval embeddings\n",
        "            if eval_embds_path is not None and os.path.exists(eval_embds_path):\n",
        "                if self.verbose:\n",
        "                    print(f\"[Frechet Audio Distance] Loading embeddings from {eval_embds_path}...\")\n",
        "                embds_eval = np.load(eval_embds_path)\n",
        "            else:\n",
        "                audio_eval = self.__load_audio_files(eval_dir, dtype=dtype)\n",
        "                embds_eval = self.get_embeddings(audio_eval, sr=self.sample_rate)\n",
        "                if eval_embds_path:\n",
        "                    os.makedirs(os.path.dirname(eval_embds_path), exist_ok=True)\n",
        "                    np.save(eval_embds_path, embds_eval)\n",
        "\n",
        "            # Check if embeddings are empty\n",
        "            if len(embds_background) == 0:\n",
        "                print(\"[Frechet Audio Distance] background set dir is empty, exiting...\")\n",
        "                return -1\n",
        "            if len(embds_eval) == 0:\n",
        "                print(\"[Frechet Audio Distance] eval set dir is empty, exiting...\")\n",
        "                return -1\n",
        "\n",
        "            # Compute statistics and FAD score\n",
        "            mu_background, sigma_background = self.calculate_embd_statistics(embds_background)\n",
        "            mu_eval, sigma_eval = self.calculate_embd_statistics(embds_eval)\n",
        "\n",
        "            fad_score = self.calculate_frechet_distance(\n",
        "                mu_background,\n",
        "                sigma_background,\n",
        "                mu_eval,\n",
        "                sigma_eval\n",
        "            )\n",
        "\n",
        "            return fad_score\n",
        "        except Exception as e:\n",
        "            print(f\"[Frechet Audio Distance] An error occurred: {e}\")\n",
        "            return -1\n"
      ],
      "metadata": {
        "id": "ZqMGK0UktIvg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0jXEL5P1sfts"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT8r8FRKsftu"
      },
      "source": [
        "### VGGISH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8RLk9FCsftu",
        "outputId": "dafbc4cb-57f2-4e5a-835e-715b046987b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in ../checkpoints/vggish/harritaylor_torchvggish_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating: background/sin_100.wav with 96000 samples.\n",
            "Creating: background/sin_200.wav with 96000 samples.\n",
            "Creating: background/sin_300.wav with 96000 samples.\n",
            "Creating: background/sin_400.wav with 96000 samples.\n",
            "Creating: background/sin_500.wav with 96000 samples.\n",
            "Creating: background/sin_600.wav with 96000 samples.\n",
            "Creating: background/sin_700.wav with 96000 samples.\n",
            "Creating: background/sin_800.wav with 96000 samples.\n",
            "Creating: background/sin_900.wav with 96000 samples.\n",
            "Creating: background/sin_1000.wav with 96000 samples.\n",
            "Creating: test1/sin_100.wav with 96000 samples.\n",
            "Creating: test1/sin_325.wav with 96000 samples.\n",
            "Creating: test1/sin_550.wav with 96000 samples.\n",
            "Creating: test1/sin_775.wav with 96000 samples.\n",
            "Creating: test1/sin_1000.wav with 96000 samples.\n",
            "Creating: test2/sin_100.wav with 96000 samples.\n",
            "Creating: test2/sin_325.wav with 96000 samples.\n",
            "Creating: test2/sin_550.wav with 96000 samples.\n",
            "Creating: test2/sin_775.wav with 96000 samples.\n",
            "Creating: test2/sin_1000.wav with 96000 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 398.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] Loading audio from background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] get_embeddings throw an exception: 'VGGish' object has no attribute 'sample_rate'\n",
            "[Frechet Audio Distance] An error occurred: need at least one array to concatenate\n",
            "FAD score test 1: -1.00000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 389.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] Loading audio from background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] get_embeddings throw an exception: 'VGGish' object has no attribute 'sample_rate'\n",
            "[Frechet Audio Distance] An error occurred: need at least one array to concatenate\n",
            "FAD score test 2: -1.00000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# STANDARD\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "frechet = FrechetAudioDistance(\n",
        "    ckpt_dir=\"../checkpoints/vggish\",\n",
        "    model_name=\"vggish\",\n",
        "    # submodel_name=\"630k-audioset\", # for CLAP only\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    use_pca=False, # for VGGish only\n",
        "    use_activation=False, # for VGGish only\n",
        "    verbose=True,\n",
        "    audio_load_worker=8,\n",
        "    # enable_fusion=False, # for CLAP only\n",
        ")\n",
        "\n",
        "for traget, count, param in [(\"background\", 10, None), (\"test1\", 5, 0.0001), (\"test2\", 5, 0.00001)]:\n",
        "    os.makedirs(traget, exist_ok=True)\n",
        "    frequencies = np.linspace(100, 1000, count).tolist()\n",
        "    for freq in frequencies:\n",
        "        samples = gen_sine_wave(freq, param=param)\n",
        "        filename = os.path.join(traget, \"sin_%.0f.wav\" % freq)\n",
        "        print(\"Creating: %s with %i samples.\" % (filename, samples.shape[0]))\n",
        "        sf.write(filename, samples, SAMPLE_RATE, \"PCM_24\")\n",
        "\n",
        "fad_score = frechet.score(\"background\", \"test1\")\n",
        "print(\"FAD score test 1: %.8f\" % fad_score)\n",
        "\n",
        "fad_score = frechet.score(\"background\", \"test2\")\n",
        "print(\"FAD score test 2: %.8f\" % fad_score)\n",
        "\n",
        "shutil.rmtree(\"background\")\n",
        "shutil.rmtree(\"test1\")\n",
        "shutil.rmtree(\"test2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPeFwd6Isftv",
        "outputId": "604a91c0-bd5d-49a4-fbe2-41dce9cf2d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in ../checkpoints/vggish/harritaylor_torchvggish_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating: background/sin_100.wav with 96000 samples.\n",
            "Creating: background/sin_200.wav with 96000 samples.\n",
            "Creating: background/sin_300.wav with 96000 samples.\n",
            "Creating: background/sin_400.wav with 96000 samples.\n",
            "Creating: background/sin_500.wav with 96000 samples.\n",
            "Creating: background/sin_600.wav with 96000 samples.\n",
            "Creating: background/sin_700.wav with 96000 samples.\n",
            "Creating: background/sin_800.wav with 96000 samples.\n",
            "Creating: background/sin_900.wav with 96000 samples.\n",
            "Creating: background/sin_1000.wav with 96000 samples.\n",
            "Creating: test1/sin_100.wav with 96000 samples.\n",
            "Creating: test1/sin_325.wav with 96000 samples.\n",
            "Creating: test1/sin_550.wav with 96000 samples.\n",
            "Creating: test1/sin_775.wav with 96000 samples.\n",
            "Creating: test1/sin_1000.wav with 96000 samples.\n",
            "Creating: test2/sin_100.wav with 96000 samples.\n",
            "Creating: test2/sin_325.wav with 96000 samples.\n",
            "Creating: test2/sin_550.wav with 96000 samples.\n",
            "Creating: test2/sin_775.wav with 96000 samples.\n",
            "Creating: test2/sin_1000.wav with 96000 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 639.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] Loading audio from background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] get_embeddings throw an exception: 'VGGish' object has no attribute 'sample_rate'\n",
            "[Frechet Audio Distance] An error occurred: need at least one array to concatenate\n",
            "FAD score test 1: -1.00000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 596.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] Loading audio from background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] get_embeddings throw an exception: 'VGGish' object has no attribute 'sample_rate'\n",
            "[Frechet Audio Distance] An error occurred: need at least one array to concatenate\n",
            "FAD score test 2: -1.00000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# with PCA\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "frechet = FrechetAudioDistance(\n",
        "    ckpt_dir=\"../checkpoints/vggish\",\n",
        "    model_name=\"vggish\",\n",
        "    submodel_name=\"630k-audioset\", # for CLAP only\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    use_pca=True, # for VGGish only\n",
        "    use_activation=False, # for VGGish only\n",
        "    verbose=True,\n",
        "    audio_load_worker=8,\n",
        "    enable_fusion=False, # for CLAP only\n",
        ")\n",
        "\n",
        "for traget, count, param in [(\"background\", 10, None), (\"test1\", 5, 0.0001), (\"test2\", 5, 0.00001)]:\n",
        "    os.makedirs(traget, exist_ok=True)\n",
        "    frequencies = np.linspace(100, 1000, count).tolist()\n",
        "    for freq in frequencies:\n",
        "        samples = gen_sine_wave(freq, param=param)\n",
        "        filename = os.path.join(traget, \"sin_%.0f.wav\" % freq)\n",
        "        print(\"Creating: %s with %i samples.\" % (filename, samples.shape[0]))\n",
        "        sf.write(filename, samples, SAMPLE_RATE, \"PCM_24\")\n",
        "\n",
        "fad_score = frechet.score(\"background\", \"test1\")\n",
        "print(\"FAD score test 1: %.8f\" % fad_score)\n",
        "\n",
        "fad_score = frechet.score(\"background\", \"test2\")\n",
        "print(\"FAD score test 2: %.8f\" % fad_score)\n",
        "\n",
        "shutil.rmtree(\"background\")\n",
        "shutil.rmtree(\"test1\")\n",
        "shutil.rmtree(\"test2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ7ipej4sftv",
        "outputId": "0ec97bfc-8819-4263-e655-ed6427673d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in ../checkpoints/vggish/harritaylor_torchvggish_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating: background/sin_100.wav with 96000 samples.\n",
            "Creating: background/sin_200.wav with 96000 samples.\n",
            "Creating: background/sin_300.wav with 96000 samples.\n",
            "Creating: background/sin_400.wav with 96000 samples.\n",
            "Creating: background/sin_500.wav with 96000 samples.\n",
            "Creating: background/sin_600.wav with 96000 samples.\n",
            "Creating: background/sin_700.wav with 96000 samples.\n",
            "Creating: background/sin_800.wav with 96000 samples.\n",
            "Creating: background/sin_900.wav with 96000 samples.\n",
            "Creating: background/sin_1000.wav with 96000 samples.\n",
            "Creating: test1/sin_100.wav with 96000 samples.\n",
            "Creating: test1/sin_325.wav with 96000 samples.\n",
            "Creating: test1/sin_550.wav with 96000 samples.\n",
            "Creating: test1/sin_775.wav with 96000 samples.\n",
            "Creating: test1/sin_1000.wav with 96000 samples.\n",
            "Creating: test2/sin_100.wav with 96000 samples.\n",
            "Creating: test2/sin_325.wav with 96000 samples.\n",
            "Creating: test2/sin_550.wav with 96000 samples.\n",
            "Creating: test2/sin_775.wav with 96000 samples.\n",
            "Creating: test2/sin_1000.wav with 96000 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 503.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] Loading audio from background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] get_embeddings throw an exception: 'VGGish' object has no attribute 'sample_rate'\n",
            "[Frechet Audio Distance] An error occurred: need at least one array to concatenate\n",
            "FAD score test 1: -1.00000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 344.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] Loading audio from background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Frechet Audio Distance] get_embeddings throw an exception: 'VGGish' object has no attribute 'sample_rate'\n",
            "[Frechet Audio Distance] An error occurred: need at least one array to concatenate\n",
            "FAD score test 2: -1.00000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# with ACTIVATIONS\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "frechet = FrechetAudioDistance(\n",
        "    ckpt_dir=\"../checkpoints/vggish\",\n",
        "    model_name=\"vggish\",\n",
        "    submodel_name=\"630k-audioset\", # for CLAP only\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    use_pca=False, # for VGGish only\n",
        "    use_activation=True, # for VGGish only\n",
        "    verbose=True,\n",
        "    audio_load_worker=8,\n",
        "    enable_fusion=False, # for CLAP only\n",
        ")\n",
        "\n",
        "for traget, count, param in [(\"background\", 10, None), (\"test1\", 5, 0.0001), (\"test2\", 5, 0.00001)]:\n",
        "    os.makedirs(traget, exist_ok=True)\n",
        "    frequencies = np.linspace(100, 1000, count).tolist()\n",
        "    for freq in frequencies:\n",
        "        samples = gen_sine_wave(freq, param=param)\n",
        "        filename = os.path.join(traget, \"sin_%.0f.wav\" % freq)\n",
        "        print(\"Creating: %s with %i samples.\" % (filename, samples.shape[0]))\n",
        "        sf.write(filename, samples, SAMPLE_RATE, \"PCM_24\")\n",
        "\n",
        "fad_score = frechet.score(\"background\", \"test1\")\n",
        "print(\"FAD score test 1: %.8f\" % fad_score)\n",
        "\n",
        "fad_score = frechet.score(\"background\", \"test2\")\n",
        "print(\"FAD score test 2: %.8f\" % fad_score)\n",
        "\n",
        "shutil.rmtree(\"background\")\n",
        "shutil.rmtree(\"test1\")\n",
        "shutil.rmtree(\"test2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz3gATPjsftz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}